---
title: 'A Dip into Machine Learning Applications in Particle Physics'
date: 2021-02-17
permalink: /posts/2021/02/blog-machine-learning-particle-physics/
tags:
  - physics
  - data science
  - machine learning
---

Note: This blog is currently a work in progress.

![](/images/particle_investigation_blog/ColorfolParticlesImage.png)


As part of the Metis data science boot camp requirements, we were required to investigate a topic and then present on it. With my physics background detailed on the main page, I was interested in learning a little bit more about machine learning applications in particle physics. While I never did any machine learning work as part of my graduate degree, and the experiment I was on didn't do any machine learning, it's always been interesting to me so I thought I would take advantage of the opportunity.

Please note that I am not an expert in this subject. Most of the material I will go through here has come from a lot of online searches, with one fantastic source being the [Living Review of Machine Learning for Particle Physics](https://iml-wg.github.io/HEPML-LivingReview/) I found on GitHub, which contains a wealth of links to various papers. At the end of this post is included a list of presentations I found very educational, which I took some inspiration (and a few plots) from. If you spot any misrepresentations in the material below, please do not hesitate to reach out to me so that I can correct them.




<!-- Some other fantastic presentations include [this one](https://indico.in2p3.fr/event/17429/attachments/47800/62191/2018-06-22_ML_Introduction_Haake.pdf) by R. Haake and [this one](https://indico.fnal.gov/event/15893/contributions/34315/attachments/21384/26602/Fermilab_ML_Lecture1.pdf) by M. Williams.
 -->


## What is particle physics and why use machine learning?
<!-- ====== -->

Particle physics is the study of the basic blocks of matter. While there are smaller experiments located around the world, the majority of research is driven by large accelerators and particle colliders. These accelerators crash particles into each other or into some material in order to produce new interesting particles and interactions. By measuring what happens after the collisions with various **experiments**, physicists can derive the underlying laws which govern our universe. Collisions happen many times a second as these accelerators run, and the experiments gather an astronomical amount of data every second. And as any one learning about the data science and machine learning world knows, a wealth of data is exactly where machine learning becomes really useful.

![](/images/particle_investigation_blog/CERN_Tunnel.png)

In general my feeling for the physics community is that there is somewhat of a split in attitudes as to the acceptance of machine learning methods. Some people are against it in favor of the more traditional "physics-based" modeling, and argue against the black-box nature of many machine learning models. Others are for it and have put it to good use, and there is no denying it's effectiveness. While understanding the physical and mathematical motivations behind any utilized models will always be necessary, as more and more research is done into the subject and the community-wide understanding grows around machine learning methods, they will make their way into more and more applications.


From my research into the subject, most of the active research has been done in the last few years, with the earliest examples going back ten years or so. Most of the machine learning methods appear to be supervised models, which is unsurprising to me as supervised learning is where every industry and application starts before moving on to unsupervised techniques. As for the techniques themselves, most of the earlier models were along the lines of decision tree ensembles, while more of the recent models are some type of neural network or other. 

<!-- physics is more of a deductive science than an inductive one
 -->

## Applications

From here on out I'll go through a few use cases. Keep in mind that the material here is by no means exhaustive. 

<!-- For a lot of my examples I've taken images and plots from various public papers and presentations and include the respective citations. -->



### Triggering and filtering / event selection via classification

If the experiments mentioned up above gathered all data that was produced, the data rate would be of order peta-bytes of data per second. In order to reduce this to a more manageable level, giga-bytes per second (still a lot!), the data is filtered with various trigger systems which decide whether or not to keep the data event by event. Most trigger systems currently use traditional, hard-cut, techniques on some parameter like the transverse-momentum, but some use machine learning classification methods, and more are planning on doing so. The power and flexibility of these new methods open up new opportunities for improved data taking and selection. As any good scientist knows, getting the useful data is often the hardest part. By improving the pipeline at the very beginning, every stage later on is improved.


![](/images/particle_investigation_blog/CMS_Event.png)


The experiments at CERN all use or will use machine learning methods in their triggering systems. 


The LHCb experiment in particular uses a neural network to remove "fake" and uninteresting events. (And in general appears to be one of the fore-runner physics experiments in using machine learning in numerous ways.) The figures below show...
 -->


<!-- <p float="left">
  <img src="/images/particle_investigation_blog/LHCb_fakes.png" width="250" />
  <img src="/images/particle_investigation_blog/LHCb_backgroundrejection.png" width="250" /> 
</p> -->
<!-- http://cdsweb.cern.ch/record/2644895/files/Machine_learning_at_LHCb_-_Kazeev_-_v6%2023.10.pdf
 -->


<!-- 
Title 1             |  Title 2
:-------------------------:|:-------------------------:
![](/images/particle_investigation_blog/LHCb_fakes.png)  |  ![](/images/particle_investigation_blog/LHCb_backgroundrejection.png) 
 -->


<!-- 
| ![](/images/particle_investigation_blog/LHC_event_selection.png) |
|:-------------------------:|
| *event selection ...*| -->



<!-- ![](/images/particle_investigation_blog/AtlasTriggerConfMatrix.png)
 -->
<!-- https://inspirehep.net/files/99e3dd9b19136c5950bff00caa0c87e7
 -->



### Particle and jet tagging via classification

Beyond machine learning methods for event selection, classification methods can be used for particle and jet tagging (identification). Particles incident on various detectors will leave particular signatures which can be used to identify and separate them. One major use case for these algorithms are for classifying so-called particle "jets." After a collision, many particles will be release with high momenta in a specific direction. As those particles travel outwards they will hit material creating new particles, decay to other particles, and so on with the daughters. This shower of particles is called a jet, and identifying whether data is related to a jet or not, and what the source is for a specific jet, is a prime example for the use of machine learning methods. The jet profiles below for different sources can be recognized with image recognition techniques using convolutional neural networks. Similarly, there is an analogy in the sequence of jet constituent particles to words in a sentence where the time-variation of the constituents provides valuable information, suggesting that recurrent or recursive neural networks might be useful. There is a lot of active research in these various areas.


![](/images/particle_investigation_blog/LHCb_PID.png)



![](/images/particle_investigation_blog/JetProfiles.png)
<!-- https://indico.in2p3.fr/event/17429/attachments/47800/62191/2018-06-22_ML_Introduction_Haake.pdf
 -->



### Particle/event parameter estimation via regression

Of course, there are are many use cases for regression methods in particle physics. Parameters for particles constantly need to be evaluated either in the final analysis results or in the steps leading up to it, including particle momenta, energy deposition, etc. and then the resolutions on these extracted parameters. Classical regression methods have long been used to great success, but machine learning methods may offer new insights or power where this parameter estimation is difficult. One paper I ran across was from a group of researches utilizing Bayesian neural networks to estimate simulated jet momenta and the respective statistical and systematic uncertainties on the extracted momenta. This last point deserves a bit more explanation, in that knowing the uncertainty on the measured values is just as important as the measurement itself, for without the former the latter is meaningless. Understanding how to determine measurement uncertainties dependent on the machine learning methods themselves is an active area of research. As this understanding grows and the black-box nature of these algorithms becomes less and less opaque, machine learning methods will only become more accepted and utilized.


![](/images/particle_investigation_blog/BayesianNN_momenta.png)
<!-- https://arxiv.org/pdf/2003.11099.pdf
 -->



### Faster simulation and detector reconstruction via GANs

As a last use case example, GANs (generative adversarial networks), can be used for faster simulation of particle distributions. A wide variety of particle distributions are constantly simulated as part of an experiments workload. These simulations provide an extremely valuable tool for both testing algorithms and models before deployment, and gaining insights about the operation of the experiment and the final measurement. I myself spent quite a bit of time simulating particle tracks that I could test my tracking algorithm on. In more of a machine learning language, this simply boils down to making predictions with an algorithm and then compare the output to truth. This is extremely valuable as you probably know. In the case of particle physics, these simulations usually take a very long time. This is due to both the statistics necessary to generate, usually on the order of billions or hundreds of billions of events, and the fact that every single event has to be processed independently. Geant4, the current main code to simulate particle distributions, propagates particles in some simulation and does so step by step for every single particle as it passes through objects, decays, changes momentum, etc. As one could imagine, doing it step by step like this takes quite a while. Before machine learning methods, this was the only way, since the accuracy of the simulation is often of the utmost importance. GANs however can learn to generate data with the same distributions and statistics as the input samples. This is currently used for a lot of image generation techniques, but the same principle can be applied to generating particle distributions. While kinks still have to be worked out, the opportunity for particle distribution generation that is multiple orders of magnitude faster than the current method would save thousands upon thousands of CPU (and by extension person) hours.


![](/images/particle_investigation_blog/GAN_Simulation.png)
<!-- https://indico.in2p3.fr/event/17429/attachments/47800/62191/2018-06-22_ML_Introduction_Haake.pdf
 -->



## Summary

All in all, machine learning is being used more and more in particle physics as the years go by. What was rare ten or so years ago is now becoming more common place and even vital now. Many recent results in particle physics have used machine learning methods either in a direct or indirect way. The possible gains and improvements are still being understood and tested but the possibilities are there and researchers are working on them. I'll be very interested to see what the landscape looks like ten years from now. While machine learning wasn't part of the core curriculum when I was in graduate school several years ago, (there were touches on it here and there), I suspect it will makes it's way into the curriculum more formally. Indeed with the level of expertise needed to build the machine learning models appropriate for particle physics use cases, there may come a time when specific degree tracks are more aligned with it.

some final sentence to cap things off...


<!--  As more time passes and the possible gains and improvements become more understood by various researchers
 -->

<!-- maybe talk briefly about kaggle competitions?
 -->



![](/images/particle_investigation_blog/Atlas_Jets.png)



List of presentations:

- [Introduction to Machine Learning in High-Energy Physics](https://indico.in2p3.fr/event/17429/attachments/47800/62191/2018-06-22_ML_Introduction_Haake.pdf) by R. Haake
- [Machine Learning at LHCb](http://cdsweb.cern.ch/record/2644895/files/Machine_learning_at_LHCb_-_Kazeev_-_v6%2023.10.pdf) by N. Kazeev
- [Machine Learning in Atalas](https://indico.cern.ch/event/673207/contributions/2755302/attachments/1560980/2457386/synpa_2017.pdf) by S. Thais
- [Machine Learning in Particle Physics](https://indico.fnal.gov/event/15893/contributions/34315/attachments/21384/26602/Fermilab_ML_Lecture1.pdf) by M. Williams




